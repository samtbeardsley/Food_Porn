{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "import datetime as dt\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from skimage import io\n",
    "from skimage.transform import resize\n",
    "from skimage import img_as_ubyte\n",
    "from skimage import color\n",
    "from tqdm import tqdm\n",
    "from shutil import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection\n",
    "\n",
    "First I collect the data from each subreddit gathering the post title, score, id, image URL, post URL, number of comments and creation timestamp. Reddit caches all of its content, so with both scraping and Reddit's API (PRAW) you're limited to 1,000 posts. Fortunately the /r/datasets community created their own API called [Pushshift](https://github.com/pushshift/api). Pushshift also has a limit of 100 posts per call, but what it has over the other methods is that you can specify a time period which allows us to bypass the limitations of caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPushshiftData(subreddit, start, end=time.time(), size=100):\n",
    "    \n",
    "    post_dict = { \"title\":[], \"created\":[], \"id\":[], \"img_url\":[], \"full_link\":[], \"score\":[], \"num_comments\":[]}\n",
    "    \n",
    "    time_list = []\n",
    "    n = start \n",
    "    day = 86400 # epoch time\n",
    "        \n",
    "    while n < end:\n",
    "        for period in range(int((end - start)/day)):\n",
    "            time_list.append([start+day*period, start+day*(period+1)])\n",
    "            n += day\n",
    "    \n",
    "    for date in time_list:\n",
    "        url = 'https://api.pushshift.io/reddit/search/submission/?&size='+str(size)+'&after='+str(date[0])+'&before='+str(date[1])+'&subreddit='+str(subreddit)\n",
    "        r = requests.get(url)\n",
    "        js = json.loads(r.text)\n",
    "        \n",
    "        for post in range(len(js['data'])):\n",
    "            post_dict['title'].append(js['data'][post]['title'])\n",
    "            post_dict['score'].append(js['data'][post]['score'])\n",
    "            post_dict['id'].append(js['data'][post]['id'])\n",
    "            post_dict['img_url'].append(js['data'][post]['url'])\n",
    "            post_dict['full_link'].append(js['data'][post]['full_link'])\n",
    "            post_dict['num_comments'].append(js['data'][post]['num_comments'])\n",
    "            post_dict['created'].append(dt.datetime.fromtimestamp(js['data'][post]['created_utc']))\n",
    "            \n",
    "        time.sleep(1)\n",
    "            \n",
    "    subreddit_df = pd.DataFrame(data=post_dict)\n",
    "    \n",
    "    return subreddit_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "subreddit = 'foodporn'\n",
    "start = 1420070400 # 12:00 am jan 1, 2015\n",
    "\n",
    "ps_fp = getPushshiftData(subreddit, start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_fp.to_csv('Data/pushshift_fp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>created</th>\n",
       "      <th>id</th>\n",
       "      <th>img_url</th>\n",
       "      <th>full_link</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My new years eve dinner</td>\n",
       "      <td>2014-12-31 18:24:30</td>\n",
       "      <td>2qyuqt</td>\n",
       "      <td>http://i.imgur.com/DgOnc4V.jpg</td>\n",
       "      <td>https://www.reddit.com/r/FoodPorn/comments/2qy...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drambuie-Butterscotch-filled Banana Cupcakes w...</td>\n",
       "      <td>2014-12-31 18:32:40</td>\n",
       "      <td>2qyvif</td>\n",
       "      <td>http://i.imgur.com/RmP3y5e.jpg</td>\n",
       "      <td>https://www.reddit.com/r/FoodPorn/comments/2qy...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Made Coquilles St. Jacques as a farewell to 20...</td>\n",
       "      <td>2014-12-31 18:57:17</td>\n",
       "      <td>2qyxmm</td>\n",
       "      <td>http://i.imgur.com/ulGNzzQ.jpg</td>\n",
       "      <td>https://www.reddit.com/r/FoodPorn/comments/2qy...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Some chocolate mint cake? Yes, please! [OC] [7...</td>\n",
       "      <td>2014-12-31 19:08:16</td>\n",
       "      <td>2qyyjq</td>\n",
       "      <td>http://i.imgur.com/ocbQx9B.jpg</td>\n",
       "      <td>https://www.reddit.com/r/FoodPorn/comments/2qy...</td>\n",
       "      <td>108</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>75$ Burger that my friend bought at an NBA gam...</td>\n",
       "      <td>2014-12-31 19:10:21</td>\n",
       "      <td>2qyyqf</td>\n",
       "      <td>http://imgur.com/gallery/2oeFCmm</td>\n",
       "      <td>https://www.reddit.com/r/FoodPorn/comments/2qy...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title             created  \\\n",
       "0                            My new years eve dinner 2014-12-31 18:24:30   \n",
       "1  Drambuie-Butterscotch-filled Banana Cupcakes w... 2014-12-31 18:32:40   \n",
       "2  Made Coquilles St. Jacques as a farewell to 20... 2014-12-31 18:57:17   \n",
       "3  Some chocolate mint cake? Yes, please! [OC] [7... 2014-12-31 19:08:16   \n",
       "4  75$ Burger that my friend bought at an NBA gam... 2014-12-31 19:10:21   \n",
       "\n",
       "       id                           img_url  \\\n",
       "0  2qyuqt    http://i.imgur.com/DgOnc4V.jpg   \n",
       "1  2qyvif    http://i.imgur.com/RmP3y5e.jpg   \n",
       "2  2qyxmm    http://i.imgur.com/ulGNzzQ.jpg   \n",
       "3  2qyyjq    http://i.imgur.com/ocbQx9B.jpg   \n",
       "4  2qyyqf  http://imgur.com/gallery/2oeFCmm   \n",
       "\n",
       "                                           full_link  score  num_comments  \n",
       "0  https://www.reddit.com/r/FoodPorn/comments/2qy...      2             1  \n",
       "1  https://www.reddit.com/r/FoodPorn/comments/2qy...      1             1  \n",
       "2  https://www.reddit.com/r/FoodPorn/comments/2qy...      1             1  \n",
       "3  https://www.reddit.com/r/FoodPorn/comments/2qy...    108             2  \n",
       "4  https://www.reddit.com/r/FoodPorn/comments/2qy...      1             2  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps_fp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 333866 entries, 0 to 333865\n",
      "Data columns (total 7 columns):\n",
      " #   Column        Non-Null Count   Dtype         \n",
      "---  ------        --------------   -----         \n",
      " 0   title         333866 non-null  object        \n",
      " 1   created       333866 non-null  datetime64[ns]\n",
      " 2   id            333866 non-null  object        \n",
      " 3   img_url       333866 non-null  object        \n",
      " 4   full_link     333866 non-null  object        \n",
      " 5   score         333866 non-null  int64         \n",
      " 6   num_comments  333866 non-null  int64         \n",
      "dtypes: datetime64[ns](1), int64(2), object(4)\n",
      "memory usage: 17.8+ MB\n"
     ]
    }
   ],
   "source": [
    "ps_fp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit = 'shittyfoodporn'\n",
    "start = 1420070400 # 12:00 am jan 1, 2015\n",
    "\n",
    "ps_sfp = getPushshiftData(subreddit, start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_sfp.to_csv('Data/pushshift_sfp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>created</th>\n",
       "      <th>id</th>\n",
       "      <th>img_url</th>\n",
       "      <th>full_link</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>was craving sushi</td>\n",
       "      <td>2014-12-31 18:01:49</td>\n",
       "      <td>2qysew</td>\n",
       "      <td>http://i.imgur.com/LS8qi14.jpg</td>\n",
       "      <td>https://www.reddit.com/r/shittyfoodporn/commen...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I had leftover Del Taco from yesterday. This i...</td>\n",
       "      <td>2014-12-31 18:07:04</td>\n",
       "      <td>2qyszb</td>\n",
       "      <td>http://i.imgur.com/Hz7yLZR.jpg</td>\n",
       "      <td>https://www.reddit.com/r/shittyfoodporn/commen...</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A special New Years dinner of grilled cheese a...</td>\n",
       "      <td>2014-12-31 18:41:38</td>\n",
       "      <td>2qywbp</td>\n",
       "      <td>http://i.imgur.com/ik8MyrX.jpg</td>\n",
       "      <td>https://www.reddit.com/r/shittyfoodporn/commen...</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nailed it!</td>\n",
       "      <td>2014-12-31 18:52:59</td>\n",
       "      <td>2qyxad</td>\n",
       "      <td>http://i.imgur.com/4ULEOyC.jpg</td>\n",
       "      <td>https://www.reddit.com/r/shittyfoodporn/commen...</td>\n",
       "      <td>131</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Some 'Snacks'</td>\n",
       "      <td>2014-12-31 18:56:20</td>\n",
       "      <td>2qyxk1</td>\n",
       "      <td>http://i.imgur.com/UY3nodm.jpg</td>\n",
       "      <td>https://www.reddit.com/r/shittyfoodporn/commen...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title             created  \\\n",
       "0                                  was craving sushi 2014-12-31 18:01:49   \n",
       "1  I had leftover Del Taco from yesterday. This i... 2014-12-31 18:07:04   \n",
       "2  A special New Years dinner of grilled cheese a... 2014-12-31 18:41:38   \n",
       "3                                         Nailed it! 2014-12-31 18:52:59   \n",
       "4                                      Some 'Snacks' 2014-12-31 18:56:20   \n",
       "\n",
       "       id                         img_url  \\\n",
       "0  2qysew  http://i.imgur.com/LS8qi14.jpg   \n",
       "1  2qyszb  http://i.imgur.com/Hz7yLZR.jpg   \n",
       "2  2qywbp  http://i.imgur.com/ik8MyrX.jpg   \n",
       "3  2qyxad  http://i.imgur.com/4ULEOyC.jpg   \n",
       "4  2qyxk1  http://i.imgur.com/UY3nodm.jpg   \n",
       "\n",
       "                                           full_link  score  num_comments  \n",
       "0  https://www.reddit.com/r/shittyfoodporn/commen...      0             1  \n",
       "1  https://www.reddit.com/r/shittyfoodporn/commen...      8             3  \n",
       "2  https://www.reddit.com/r/shittyfoodporn/commen...      6             2  \n",
       "3  https://www.reddit.com/r/shittyfoodporn/commen...    131            15  \n",
       "4  https://www.reddit.com/r/shittyfoodporn/commen...      1             0  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps_sfp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 317486 entries, 0 to 317485\n",
      "Data columns (total 7 columns):\n",
      " #   Column        Non-Null Count   Dtype         \n",
      "---  ------        --------------   -----         \n",
      " 0   title         317486 non-null  object        \n",
      " 1   created       317486 non-null  datetime64[ns]\n",
      " 2   id            317486 non-null  object        \n",
      " 3   img_url       317486 non-null  object        \n",
      " 4   full_link     317486 non-null  object        \n",
      " 5   score         317486 non-null  int64         \n",
      " 6   num_comments  317486 non-null  int64         \n",
      "dtypes: datetime64[ns](1), int64(2), object(4)\n",
      "memory usage: 17.0+ MB\n"
     ]
    }
   ],
   "source": [
    "ps_sfp.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking great so far!  I'll need to check for duplicates and also 600,000+ images is more than I can store, so I'll determine a cutoff based on score to get the \"most representative\" images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_unique = ps_fp.drop_duplicates(keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 166934 entries, 0 to 333382\n",
      "Data columns (total 7 columns):\n",
      " #   Column        Non-Null Count   Dtype         \n",
      "---  ------        --------------   -----         \n",
      " 0   title         166934 non-null  object        \n",
      " 1   created       166934 non-null  datetime64[ns]\n",
      " 2   id            166934 non-null  object        \n",
      " 3   img_url       166934 non-null  object        \n",
      " 4   full_link     166934 non-null  object        \n",
      " 5   score         166934 non-null  int64         \n",
      " 6   num_comments  166934 non-null  int64         \n",
      "dtypes: datetime64[ns](1), int64(2), object(4)\n",
      "memory usage: 10.2+ MB\n"
     ]
    }
   ],
   "source": [
    "fp_unique.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfp_unique = ps_sfp.drop_duplicates(keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 158743 entries, 0 to 158742\n",
      "Data columns (total 7 columns):\n",
      " #   Column        Non-Null Count   Dtype         \n",
      "---  ------        --------------   -----         \n",
      " 0   title         158743 non-null  object        \n",
      " 1   created       158743 non-null  datetime64[ns]\n",
      " 2   id            158743 non-null  object        \n",
      " 3   img_url       158743 non-null  object        \n",
      " 4   full_link     158743 non-null  object        \n",
      " 5   score         158743 non-null  int64         \n",
      " 6   num_comments  158743 non-null  int64         \n",
      "dtypes: datetime64[ns](1), int64(2), object(4)\n",
      "memory usage: 9.7+ MB\n"
     ]
    }
   ],
   "source": [
    "sfp_unique.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 0.1 quantile has 71614 fp images\n",
      "the 0.2 quantile has 71614 fp images\n",
      "the 0.30000000000000004 quantile has 71614 fp images\n",
      "the 0.4 quantile has 71614 fp images\n",
      "the 0.5 quantile has 71614 fp images\n",
      "the 0.6 quantile has 66695 fp images\n",
      "the 0.7000000000000001 quantile has 49954 fp images\n",
      "the 0.8 quantile has 33291 fp images\n",
      "the 0.9 quantile has 16546 fp images\n"
     ]
    }
   ],
   "source": [
    "for quant in np.arange(0.1,1,0.1):\n",
    "    print(f'the {quant} quantile has {len(fp_unique.score[fp_unique.score > fp_unique.score.quantile(quant)])} fp images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 0.1 quantile has 96341 sfp images\n",
      "the 0.2 quantile has 96341 sfp images\n",
      "the 0.30000000000000004 quantile has 96341 sfp images\n",
      "the 0.4 quantile has 91149 sfp images\n",
      "the 0.5 quantile has 77104 sfp images\n",
      "the 0.6 quantile has 60393 sfp images\n",
      "the 0.7000000000000001 quantile has 46420 sfp images\n",
      "the 0.8 quantile has 31388 sfp images\n",
      "the 0.9 quantile has 15775 sfp images\n"
     ]
    }
   ],
   "source": [
    "for quant in np.arange(0.1,1,0.1):\n",
    "    print(f'the {quant} quantile has {len(sfp_unique.score[sfp_unique.score > sfp_unique.score.quantile(quant)])} sfp images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~30,000 images each is much more reasonable, so I'll use the 80th quantile. I also anticipate that this will be reduced more due to not every url being a png or jpg as videos and gifs are allowed. Additionally, since I'm looking back over 5 years, there will be images that have been removed or deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp.to_csv('Data/fp.csv')\n",
    "sfp.to_csv('Data/sfp.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting The Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have the urls, its time to get the images. The below function downloads the images from the subreddit dataframes created from the pushshift API and gives them unique names based on the id column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_subreddit_images(subreddit_df, directory):\n",
    "    \"\"\"downloads all static images (png & jpg) from specified subreddit dataframe to desired directory\"\"\"\n",
    "    \n",
    "    headers = {'user-agent': 'image_downloader'}\n",
    "    url_dict = {}\n",
    "    skipped_urls = []\n",
    "    \n",
    "    # checks if directory exists, if not makes one\n",
    "    current_path = os.getcwd()\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    os.chdir(directory)\n",
    "    \n",
    "    # find urls and create file names from unique post id\n",
    "    for row in subreddit_df.itertuples():\n",
    "        filename = f\"{row[3]}.{str.split(str(row[4]), sep='.')[-1]}\"\n",
    "        url =row[4]\n",
    "        url_dict[filename] = url\n",
    "    \n",
    "    # iterates over static images in dict and saves to file\n",
    "    for filename, url in tqdm(url_dict.items()):\n",
    "        # check if url is png or jpg\n",
    "        if (str.split(filename, sep='.')[-1] == 'jpg') or (str.split(filename, sep='.')[-1] =='png'):\n",
    "            try:\n",
    "                r = requests.get(url, headers=headers, stream=True)\n",
    "       \n",
    "                with open(filename, 'wb') as fd:\n",
    "                    for chunk in r.iter_content(chunk_size=128):\n",
    "                        fd.write(chunk)\n",
    "            except:\n",
    "                skipped_urls.append(url)\n",
    "            time.sleep(3)\n",
    "        pass\n",
    "    \n",
    "    os.chdir(current_path)\n",
    "    return skipped_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33291/33291 [28:07:49<00:00,  3.04s/it]    \n"
     ]
    }
   ],
   "source": [
    "fp_skipped = download_subreddit_images(fp,'Images/FP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31388/31388 [23:55:05<00:00,  2.74s/it]    \n"
     ]
    }
   ],
   "source": [
    "sfp_skipped = download_subreddit_images(sfp,'Images/SFP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Data From Images\n",
    "\n",
    "Now that the images are downloaded, 24,746 from FP and 20,453 from SFP, I want to get some basic info on them such as the heigh and witdth, aspect ratio, and the mean/standard deviation of the color channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_dict(path):\n",
    "    \"\"\" create dictionary of statistics for each image, heigh, width, aspect ratio,\n",
    "    and mean/standard deviation of red, green, blue color channels.\n",
    "    Images causing error are skipped and placed into a list for additional inspection\"\"\"\n",
    "        \n",
    "    image_dict = {'path':[], 'height':[], 'width':[], 'ratio_(w/h)':[],\n",
    "                 'mean_red':[], 'mean_green': [], 'mean_blue':[],\n",
    "                 'std_red':[], 'std_green':[], 'std_blue':[]}\n",
    "\n",
    "    # some images are throwing an error when extracting data, place them here to see whats going on\n",
    "    bad_images = []     \n",
    "\n",
    "    img_names = os.listdir(path)\n",
    "    for image in tqdm(img_names):\n",
    "        try:\n",
    "            img = io.imread(f'{path}/{image}')\n",
    "            red = img[:, :, 0]\n",
    "            green = img[:, :, 1]\n",
    "            blue = img[:, :, 2]\n",
    "            image_dict['height'].append(img.shape[1])\n",
    "            image_dict['width'].append(img.shape[0])\n",
    "            image_dict['ratio_(w/h)'].append(img.shape[0]/img.shape[1])\n",
    "            image_dict['mean_red'].append(np.mean(red))\n",
    "            image_dict['mean_green'].append(np.mean(green))\n",
    "            image_dict['mean_blue'].append(np.mean(blue))\n",
    "            image_dict['std_red'].append(np.std(red))\n",
    "            image_dict['std_green'].append(np.std(green))\n",
    "            image_dict['std_blue'].append(np.std(blue))\n",
    "            image_dict['path'].append(image)\n",
    "        except:\n",
    "            bad_images.append(image)\n",
    "            \n",
    "    return image_dict, bad_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build fp image dict\n",
    "fp_path = 'Images/FP'\n",
    "fp_image_dict, fp_bad_images = get_img_dict(fp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert fp image dict & bad image list to dataframes and save \n",
    "fp_image_df = pd.DataFrame(data=fp_image_dict)\n",
    "fp_image_df.to_csv('Data/fp_image_data.csv')\n",
    "\n",
    "fp_bad_image_df = pd.DataFrame(data=fp_bad_images)\n",
    "fp_bad_image_df.to_csv('Data/fp_bad_images.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20453/20453 [1:44:57<00:00,  3.25it/s]  \n"
     ]
    }
   ],
   "source": [
    "# build sfp image dict\n",
    "path = 'Images/SFP'\n",
    "sfp_image_dict, sfp_bad_images = get_img_dict(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert sfp image dict & bad image list to dataframes and save\n",
    "sfp_image_df = pd.DataFrame(data=sfp_image_dict)\n",
    "sfp_image_df.to_csv('Data/sfp_image_data.csv')\n",
    "\n",
    "sfp_bad_image_df = pd.DataFrame(data=sfp_bad_images)\n",
    "sfp_bad_image_df.to_csv('Data/sfp_bad_images.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import image data dfs\n",
    "fp_image_df = pd.read_csv('Data/fp_image_data.csv', index_col=0)\n",
    "sfp_image_df = pd.read_csv('Data/sfp_image_data.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import bad images\n",
    "fp_bad_image = pd.read_csv('Data/fp_bad_images.csv', index_col=0)\n",
    "sfp_bad_image = pd.read_csv('Data/sfp_bad_images.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I move the \"bad images\" to a new folder so I can see what the issue is. There are 1,846 from FP and 1,771 from SFP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# moving the \"bad images\" to new folders so I can look at them\n",
    "fp_bad_image.columns= ['image']\n",
    "fp_bad_image_list = list(fp_bad_image.image)\n",
    "\n",
    "for image in fp_bad_image_list:\n",
    "    os.rename(f'Images/FP/{image}', f'Images/FP_Bad_Images/{image}')\n",
    "    \n",
    "    \n",
    "sfp_bad_image.columns= ['image']\n",
    "sfp_bad_image_list = list(sfp_bad_image.image)\n",
    "for image in sfp_bad_image_list:\n",
    "    os.rename(f'Images/SFP/{image}', f'Images/SFP_Bad_Images/{image}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good new, the \"bad images\" are only the imgur notifications that the images has been removed.\n",
    "\n",
    "![example](Images/FP_Bad_Images/2r4v19.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resizing \n",
    "\n",
    "Next, I want to resize my images. I plan to use transfer learning from VGG16, which requires images to be 224x224, so that's the size I'll set my images to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_resize(image_path, new_path, size=(224,224)):\n",
    "    '''resize images to specified size and saves a copy in new path'''\n",
    "    img_to_convert = os.listdir(image_path)\n",
    "    \n",
    "    img_in_new = os.listdir(new_path)\n",
    "    \n",
    "    # this will take a while to run - if interupted this will check for images already resized\n",
    "    images = list(set(img_to_convert) - set(img_in_new))\n",
    "    \n",
    "    for image in tqdm(images):\n",
    "        img = io.imread(f'{image_path}/{image}')\n",
    "        img_resize = resize(img, (224,224))\n",
    "        try:\n",
    "            # resize and save new image\n",
    "            io.imsave(f'{new_path}/{image}', img_as_ubyte(img_resize))\n",
    "        \n",
    "        except IOError: \n",
    "            # IOError happens while saving as jpeg if the image has an alpha channel\n",
    "            img_resize = color.rgba2rgb(img_resize)\n",
    "            io.imsave(f'{new_path}/{image}', img_as_ubyte(img_resize))\n",
    "            \n",
    "        except Exception:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_path = 'Images/FP'\n",
    "new_path = 'Images/FP_224x224'\n",
    "\n",
    "custom_resize(current_path, new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_path = 'Images/SFP'\n",
    "new_path = 'Images/SFP_224x224'\n",
    "\n",
    "custom_resize(current_path, new_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, while i have a lot of images, I will cut it down to 500 per subreddit for EDA and initial training. More can be added as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_df = pd.read_csv('Data/fp.csv', index_col=0)\n",
    "sfp_df = pd.read_csv('Data/sfp.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_image_df = pd.read_csv('Data/fp_image_data.csv', index_col=0)\n",
    "sfp_image_df = pd.read_csv('Data/sfp_image_data.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split path column to merge with fp_df on id\n",
    "fp_image_df['id'] = fp_image_df['path'].str.split(pat='.',n=1).str[0]\n",
    "sfp_image_df['id'] = sfp_image_df['path'].str.split(pat='.',n=1).str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge fp_image_df with fp_df\n",
    "fp500 = fp_image_df.merge(fp_df,how='left', on='id')\n",
    "\n",
    "# sort by score and select the top 500\n",
    "fp500 = fp500.sort_values('score', ascending=False).head(500)\n",
    "\n",
    "# same for sfp data\n",
    "sfp500 = sfp_image_df.merge(sfp_df,how='left', on='id')\n",
    "sfp500 = sfp500.sort_values('score', ascending=False).head(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfp500.to_csv('Data/sfp500.csv')\n",
    "fp500.to_csv('Data/fp500.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copying the top 500 to a new folder\n",
    "for image in fp500.path:\n",
    "    copy(f'Images/FP_224x224/{image}', f'Images/FP500_224x224/{image}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image in sfp500.path:\n",
    "    copy(f'Images/SFP_224x224/{image}', f'Images/SFP500_224x224/{image}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the images are resized, I also was to get the intensity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resize_dict(path):\n",
    "    \"\"\" create dictionary of statistics for each image, heigh, width, aspect ratio,\n",
    "    and mean/standard deviation of red, green, blue color channels.\n",
    "    Images causing error are skipped and placed into a list for additional inspection\"\"\"\n",
    "        \n",
    "    image_dict = {'path':[], 'height':[], 'width':[], 'ratio_(w/h)':[], 'intensity':[],\n",
    "                 'mean_red':[], 'mean_green': [], 'mean_blue':[],\n",
    "                 'std_red':[], 'std_green':[], 'std_blue':[]}\n",
    "    \n",
    "    img_names = os.listdir(path)\n",
    "    for image in tqdm(img_names):\n",
    "        try:\n",
    "            img = io.imread(f'{path}/{image}')\n",
    "            red = img[:, :, 0]\n",
    "            green = img[:, :, 1]\n",
    "            blue = img[:, :, 2]\n",
    "            image_dict['height'].append(img.shape[1])\n",
    "            image_dict['width'].append(img.shape[0])\n",
    "            image_dict['ratio_(w/h)'].append(img.shape[0]/img.shape[1])\n",
    "            image_dict['mean_red'].append(np.mean(red))\n",
    "            image_dict['mean_green'].append(np.mean(green))\n",
    "            image_dict['mean_blue'].append(np.mean(blue))\n",
    "            image_dict['std_red'].append(np.std(red))\n",
    "            image_dict['std_green'].append(np.std(green))\n",
    "            image_dict['std_blue'].append(np.std(blue))\n",
    "            image_dict['intensity'].append(np.sum(red)+np.sum(green)+np.sum(blue))\n",
    "            image_dict['path'].append(image)\n",
    "        except Exception:\n",
    "            pass\n",
    "            \n",
    "    return image_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:01<00:00, 314.26it/s]\n"
     ]
    }
   ],
   "source": [
    "path = \"Images/FP500_224x224\"\n",
    "fp500_resize_image_df = pd.DataFrame(get_resize_dict(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:01<00:00, 329.77it/s]\n"
     ]
    }
   ],
   "source": [
    "path = \"Images/SFP500_224x224\"\n",
    "sfp500_resize_image_df = pd.DataFrame(get_resize_dict(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp500_resize_image_df.to_csv('Data/fp500_resize_data.csv')\n",
    "sfp500_resize_image_df.to_csv('Data/sfp500_resize_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
